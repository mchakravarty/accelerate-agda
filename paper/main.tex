% 16pages; IFL 2012
%
\documentclass{llncs}
%
\usepackage{amssymb}
\usepackage{cite}
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}

%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{proof}
\usepackage{alltt}


\begin{document}
%
\title{Agda Meets Accelerate}
\subtitle{Extended Abstract}
\author{Peter Thiemann\inst{1} \and Manuel M. T. Chakravarty\inst{2}}
\institute{
  University of Freiburg, Germany,\\
  \email{thiemann@informatik.uni-freiburg.de}
\and
University of New South Wales, Sydney, Australia,\\
\email{chak@cse.unsw.edu.au}
}

\maketitle              % typeset the title of the contribution

\begin{abstract}  
  Embedded languages in Haskell benefit from a range of type extensions, such as type families, that are subsumed by dependent types. However, even with those type extensions, embedded languages for data parallel programming lack desirable static guarantees, such as static bounds checks in indexing and collective permutation operations.
  
  This raises the question whether embedded languages for data parallel programming would benefit from fully-fledged dependent types, such as those available in Agda. We explored that question by designing and implementing an Agda frontend to Accelerate, an embedded language for data parallel programming aimed at GPUs. We discuss the potential of dependent types in this domain, describe some of the limitations that we encountered, and share some insights from our preliminary implementation.
\end{abstract}
\keywords{programming with dependent types, data parallelism}
\thispagestyle{plain}
\pagestyle{plain}
%
\section{Introduction}
\label{sec:introduction}

Generative approaches to programming parallel hardware promise to combine high-level programming models with high-performance. They are especially attractive for targeting restricted architectures, such as GPUs (graphics processor units), that cannot efficiently execute code aimed at conventional multicore CPUs. Instead, GPUs require a high degree of data parallelism, restricted control flow, and carefully tailored data access patterns to be efficient. Previous work ---for example, Accelerator~\cite{??}, Copperhead~\cite{??}, and Accelerate~\cite{ChakravartyKellerLeeMcdonellGrover2011}--- demonstrates that embedded array languages with a custom code generator can meet those GPU constraints by carefully limiting the embedded language constructs.

Given a host languages with an expressive type system, it is attractive to leverage that type system to express static properties of the embedded language. For example, Accelerate, an embedded array language for Haskell, uses Haskell's recent support for type-level programming like GADTs and type families in that manner~\cite{ChakravartyKellerLeeMcdonellGrover2011}. This is especially important for approaches relying on runtime code generation as we want to mimise the number of potential faults at application runtime, which coincides with the compile time of the embedded language. Moreover, static guaranties hold the potential to improve the predictability of parallel performance.

Dependent types are an emerging approach to certified programming, where invariants are established in the form of types and proved at compile time. Many of Haskell's type-level extensions used in Accelerate approximate various aspects of dependently-typed programming. Hence, it it is natural to ask whether fully-fledged dependent types, such as those provided by Agda, improve the specification of an embedded language like Accelerate, whether they increase the scope of static guarantees, and whether they may be leveraged to predict performance more accurately.

This paper is a first investigation into this topic. It reports of a
partial port of Accelerate to a new, dependently-typed host language,
Agda \cite{Norell2008,BoveDybjerNorell2009}. Agda is particularly
suited to this port because of its foreign function interface to
Haskell, which enables it to directly invoke functionality of
Accelerate. 

Our investigation has the following structure. After recalling some
background on Agda and Accelerate in Section~\ref{sec:background} and
describing related work in Section~\ref{sec:related-work}, 
Section~\ref{sec:dependent-types} discusses potential uses of
dependent types in an array-oriented data parallel language like
Accelerate. Section~\ref{sec:limitations} considers conceptual
problems and limitations that we ran into when constructing the Agda
frontend for Accelerate. Section~\ref{sec:implementation} explains
some technical details of the implementation and discusses some
example code. 

\section{Background}
\label{sec:background}

\subsection{Agda}
\label{sec:agda}

Agda \cite{Norell2008,BoveDybjerNorell2009} is a dependently typed
functional programming language. Its basis is a dependently typed
lambda calculus extended with inductive data type families, dependent
records, and parameterized modules. At the same time, Agda is also a
proof assistant for interactively constructing proofs in an
intuitionistic type theory based on the work of Per Martin-L\"of
\cite{MartinLoef1984}. 

A familiar example for an indexed data type is the type
\verb+Vec A n+ for vectors of fixed length \verb+n+ and elements of
type \verb+A+ program program with an access operation that restricts the index to the
actual length of the vector at compile time.\footnote{An
  identifier can be an almost arbitrary 
  string of Unicode characters except spaces, parentheses, and curly
  braces. Agda also supports mixfix syntax with the position of
  arguments indicated by underscores in the defining occurrence of an
  identifier.} 
\begin{verbatim}
data Nat : Set where
  zero : Nat
  suc  : Nat -> Nat

data Vec (A : Set) : Nat -> Set where
  []   : Vec A zero
  _::_ : {n : Nat} -> A -> Vec A n -> Vec A (suc n)
\end{verbatim}
The above defines an indexed data type \verb+Vec A n+ with two
constructors, \verb+[]+ for the vector of length zero and 
\verb+_::_+ for the infix cons operator that increases the length by one.

One way of writing a safe access operation first defines an indexed
type that encodes the required less-than relation on natural numbers.
\begin{verbatim}
data _<_ : Nat -> Nat -> Set where
  z<s : {n : Nat} -> zero < suc n
  s<s : {m n : Nat} -> m < n -> suc m < suc n
\end{verbatim}
Lines two and three of the definition encode named inference rules for
the cases that $0 < n+1$ (for all $n$) and that $m+1 < n+1$ if $m < n$
(for all $m,n$).

The access operation takes a vector of length \verb+n+, an index
\verb+m+, and a proof of \verb+m < n+ (a derivation tree) to produce
an element of the vector.  
\begin{verbatim}
get : {A : Set} {n : Nat} -> Vec A n -> (m : Nat) -> m < n -> A
get []        _       ()      -- impossible case
get (x :: xs) zero    p       = x
get (x :: xs) (suc m) (s<s p) = get xs m p
\end{verbatim}
This code cannot fail at run time because a caller has to
construct the proof tree for \verb+m < n+ before invoking \verb+get+.

\subsection{Accelerate}
\label{sec:accelerate}

Accelerate~\cite{ChakravartyKellerLeeMcdonellGrover2011} is a data-parallel array language embedded into Haskell, which targets GPUs. It is a \emph{generative library}, as its data-parallel array operations are not executed directly, but instead construct abstract syntax trees (AST) representing an entire data-parallel subcomputation. These \emph{computation representations} are executed using a \verb+run+ operation that accepts such an AST (of type \verb+Acc a+), compiles it to GPU kernels, uploads it to the device, executes it, and retrieves the results.
%
\begin{verbatim}
CUDA.run :: Arrays a => Acc a -> a
\end{verbatim}
%
The type class constraint \verb+Arrays a+ restricts the result type to a single array or a tuple of arrays.

As computation representations of type \verb+Acc a+ are compiled at application runtime, all \verb+Acc+ compilation errors are effectivly \emph{runtime errors} of the application. Hence, Accelerate uses a range of Haskell type system extensions to statically type Accelerate expressions, such that these runtime errors are avoided where possible. In particular, Accelerate uses GADTs\cite{PeytonJonesVytiniotisWeirichWashburn2006}, associated types
\cite{ChakravartyKellerJones2005}, and type families
\cite{SchrijversPeytonJonesChakravartySulzmann2008}. 

As a simple example of an Accelerate program, consider a function implementing a dot product:
%
\begin{verbatim}
dotp :: Vector Float -> Vector Float -> Acc (Scalar Float)
dotp xs ys = let xs' = use xs
                 ys' = use ys
             in  fold (+) 0 (zipWith (*) xs' ys')
\end{verbatim}
%
The types \verb+Vector+ and \verb+Scalar+ represent one- and zero-dimensional
arrays. Plain arrays, such as \verb+Vector Float+ are conventional Haskell arrays, using an unboxed representation to improve performance. However, when they are wrapped into the constructor \verb+Acc+, such as in \verb+Acc (Scalar Float)+, they represent arrays of the embedded language and are allocated in GPU memory, which in current high-performance GPUs is physically separate from CPU memory.

The \verb+use+ operation makes a Haskell array available in the embedded language by wrapping it into the \verb+Acc+ constructor and copying it to GPU memory.\footnote{Accelerate employs a caching strategy to avoid the transfer of arrays, which are already available in GPU memory.} The operations \texttt{fold} and \texttt{zipWith} represent collective operations on Accelerate arrays, effectively producing a representation of an array computation yielding a value of
\texttt{Scalar Float}; i.e., a single float value. This code relies
heavily on (type class) overloading: \texttt{0}, \texttt{(+)}, and
\texttt{(*)} are overloaded to just construct abstract syntax. 

The type \verb+Scalar+ and \verb+Vector+ are merely type synonyms instantiating a shape-parameterised array type to the special case of zero and one dimensional arrays:
%
\begin{verbatim}
type Scalar e = Array DIM0 e
type Vector e = Array DIM1 e
\end{verbatim}
%
So, the general type for \verb+use+ is
%
\begin{verbatim}
use :: Elt e => Array sh e -> Acc (Array sh e)
\end{verbatim}
%
where the class \verb+Elt+ characterises all types that may be held in Accelerate arrays. These are currently primitive types and tuples.

Common dimensions, such as \verb+DIM0+, \verb+DIM1+, and so on, are predefined, but to enable shape polymorphic computations, along the lines pioneered in the Haskell array library Repa~\cite{keller-etal:repa}, shapes are inductively defined using type-level snoc lists formed from
%
\begin{verbatim}
data Z       = Z
data sh :. i = sh :. i
\end{verbatim}
%
Hence, we have got
%
\begin{verbatim}
type DIM0 = Z
type DIM1 = DIM0 :. Int
<and so on>
\end{verbatim}

\section{Related Work}
\label{sec:related-work}

Peebles formalised parts of the API of the Haskell array library Repa using Agda~\cite{peebles:derpa}. The formalisation uses the same shape structure as we are using in Accelerate, but array computations are neither embedded nor can parallel high-performance code be generated.

Swierstra and Altenkirch investigated the use of dependent types for \emph{distributed} array programming~\cite{swierstra-altenkirch:dep-types-for-distr-arrays,swierstra:more-dependent-types}. Their notation for distributed arrays was inspired by X10 and the main focus is on expressing locality aware algorithms.

Dependent ML is an ML dialect with restricted form of dependent types, which, among other applications, may be used to statically check array bounds in array operations~\cite{xi:dml-jfp}. However, only simple indexing and array updating are considered and not aggregate array operations, such as those provided by Accelerate.

Accelerator~\cite{Tarditi:2006} enables embedded GPU computations in C\# programs; it subsequently also added F\# support. However, no attempt is made to track properties of array programs statically. Similarly, Copperhead~\cite{Catanzaro:EECS-2010-124} embedded an array language into Python, but does not attempt to track information statically.

\section{Dependent Types for Accelerate}
\label{sec:dependent-types}

In this section, we investigate the potential uses of dependent typing
in a language like Accelerate.

\subsection{Exact Checking of Array Bounds}
\label{sec:exact-checking-array}

Accelerate's API features expressive type constraints that describe
the shape of the array arguments and results. These constraints ensure
that no shape mismatches occur (e.g., a one-dimensional array is
considered two-dimensional). However, they do not ensure at compile
time that the sizes of the dimensions match up.

As an example, consider the function \texttt{reshape}. 
It takes a target shape \texttt{sh} and an array of source shape
\texttt{sh'} and changes the layout of that array to \texttt{sh}. 
\begin{verbatim}
reshape :: Exp sh -> Acc (Array sh' e) -> Acc (Array sh e)
\end{verbatim}
For this reshaping to work correctly, the underlying
number of elements must remain the same. For example, while it makes sense
to reshape a two-dimensional $3\times 4$-array  to a vector of size
$12$ or to a three-dimensional $3\times2\times2$-array, whereas an attempt to
reshape the same array to a $2\times5$-array should be rejected at
compile time.

In Agda, we define the shape type as follows.
\begin{verbatim}
data Shape : Set where
  Z     : Shape
  _:<_> : Shape -> Nat -> Shape
\end{verbatim}
The \texttt{size} function computes the number of elements stored in a
shape.
\begin{verbatim}
size : Shape -> Nat
size Z = 1
size (sh :< n >) = size sh * n
\end{verbatim}
Now we can state an accurate type for \texttt{reshape} in Agda, which
involves an extra argument with a proof that the source and
target shapes have the same size.\footnote{For the moment, the type
  \texttt{Element} could be read as \texttt{Set}. Actually,
  \texttt{Element} classifies the types of values that can be stored
  in Accelerate arrays. In Haskell, this restriction is imposed with a
  type class.}
\begin{verbatim}
reshape : {sh : Shape} {E : Element}
       -> (sh' : Shape) -> Array sh E -> (size sh = size sh')
       -> Array sh' E
\end{verbatim}
There is a subtle difference to the original signature. In
Accelerate, the first argument is an \emph{expression} that produces a
value of type \texttt{sh} at run time, whereas the Agda
\texttt{reshape} requires a \texttt{Shape} as its first argument.


Furthermore, functions like \texttt{map} and \texttt{zipWith} obtain
more precise types. The type of \texttt{map} tells us that the input
shape is identical to the output shape:
\begin{verbatim}
map : {A B} {sh} -> (Exp A -> Exp B) -> Array sh A -> Array sh B
\end{verbatim}
Similarly, the type of \texttt{zipWith} restricts its input arrays to
identical shapes:
\begin{verbatim}
zipWith : {A B C} {sh} -> (Exp A -> Exp B -> Exp C)
        -> Array sh A -> Array sh B -> Array sh C
\end{verbatim}
The latter type is more restrictive than the Accelerate
implementation of \texttt{zipWith}. Instead of checking the dimensions
of the input arrays, it truncates them to the respective minima. A
corresponding Agda type could be developed easily. It would involve
specifying a suitable ternary relation between shapes.

\subsection{Associativity of Operations}
\label{sec:assoc-oper}

Some parallel reduction operations require their base operation to be
associative to return a predictable result. Here are two examples from
Accelerate. 
\begin{verbatim}
fold  :: (Shape ix, Elt a) =>
         (Exp a -> Exp a -> Exp a) -> Exp a ->
         Acc (Array (ix :. Int) a) -> Acc (Array ix a)
fold1 :: (Shape ix, Elt a) =>
         (Exp a -> Exp a -> Exp a) ->
         Acc (Array (ix :. Int) a) -> Acc (Array ix a)
\end{verbatim}
In both cases, the text of the documentation says that ``the first
argument needs to be associative'' and the \texttt{fold1}
documentation ``requires the reduced array to be non-empty''.
The second requirement can be enforced by asking for a suitable proof
object on each call of \texttt{fold1}:
\begin{verbatim}
fold1 : ... -> Array (sh :< n >) E -> (p : size sh * n > 0)
            -> Array sh E
\end{verbatim}
The first requirement can be rephrased to saying that the first two
parameters of \texttt{fold} together form a monoid, which requires an
associative operation with a unit element. The concept of a monoid
can be formalized in Agda, which has indeed been done in the standard
library. Unfortunately, the formalization from the library cannot be
used because Accelerate deals with ASTs, not with values. So, a
formalization is required that states that the meaning of an
AST-encoded function is associative and the meaning of another
AST-encoded constant is its unit element. Given that Accelerate
encodes AST construction using higher-order abstract syntax, such a
formalization is not straightforward. Moreover, even given expressions
with a fixed meaning, there is no general shape for associative
functions, so that proofs can only be done for special cases. 

\subsection{Embedding of Haskell Types}
\label{sec:embedding-constants}

Accelerate supports a wide range of numeric types as base types for
array computations. Almost all of these types have no suitable
counterpart in Agda, which only supports natural numbers in unary
encoding. For that reason, our embedding keeps these types 
abstract in Agda. To specify functions that are polymorphic in such a
Haskell type or depend on it in some way, we have reified these types
in an Agda type \texttt{Element}.
\begin{verbatim}
data Element : Set where
  Bool   : Element
  Int    : Element
  Float  : Element
  Double : Element
  Pair   : Element -> Element -> Element
  ...
\end{verbatim}
Corresponding to Haskell type classes that are used in Accelerate, our
embedding supplies predicates that characterize subsets. For example,
the set of numeric type is defined by a predicate \texttt{IsNumeric}.
\begin{alltt}
IsNumeric : Element -> Set
IsNumeric Int = \(\top\)
IsNumeric Float = \(\top\)
IsNumeric Double = \(\top\)
IsNumeric _ = \(\bot\)
\end{alltt}

\subsection{Embedding of Constants}
\label{sec:embedding-constants}


Accelerate relies on Haskell's built-in support for the type classes
\texttt{Num} and \texttt{Fractional} to embed constants. The Haskell compiler reads each integer
literal as a value of type \texttt{Integer}, which is a built-in type
of arbitrary precision integers. To this value, Haskell applies the
function \texttt{fromInteger} that converts to the expected
type. Similarly, floating point constants are read as values of
type \texttt{Rational} (\texttt{Integer} fractions) and then converted
using \texttt{fromRational}. Accelerate provides instances of
these type classes that define \texttt{fromInteger} and
\texttt{fromRational} to produce suitable AST fragments.

Because of Agda's limited support for numeric data types,
we embed more ambitious numeric literals for floating point numbers
using a string with an explicit type annotation that determines the
parsing of the string. Here are some example embeddings:
\begin{verbatim}
"3.1415926" ::: Float
"6.0221415E23" ::: Double
\end{verbatim}
Recall that \texttt{Float} and \texttt{Double} are not
types, but rather values of type \texttt{Element}.
All magic of the embedding is hidden in the \texttt{:::} operation:
\begin{verbatim}
_:::_ : (s : String) -> (E : Element) 
     -> {{nu : IsNumeric E}} -> {p : T (s parsesAs E)} -> Exp E
s ::: E = Ex (constantFromString (EltDict E) (ReadDict E) s)
\end{verbatim}
The arguments \texttt{s} and \texttt{E} are explicit, but the
remaining ones are to be inferred by Agda.
The argument \texttt{nu} is an instance argument that is automatically
filled-in with a suitably typed value in scope
\cite{DevriesePiessens2011}. Here, the predicate 
\texttt{IsNumeric} plays the role of a type class that characterizes
the numeric types.

The function \texttt{parsesAs} dispatches on its ``type'' argument and
checks whether the string is a constant of the expected type. The
function \texttt{constantFromString} is imported from Accelerate.
It is an overloaded function that requires two type dictionaries,
which are computed from \texttt{E} using the functions \texttt{EltDict}
and \texttt{ReadDict}. 

\section{Limitations}
\label{sec:limitations}

In a number of places, Accelerate's generativity limits the
applicability of dependent typing. We already mentioned that the
formalization of associativity or of the concept monoid gets
unmanageable because such properties have to be asserted for abstract syntax.

For a similar problem, consider an implementation of the \texttt{filter}
operation that takes a predicate and a source array and returns an
array that only contains the elements of the source array fulfilling
the predicate.  First of all, filtering only makes sense for
one-dimensional arrays, that is, for vectors. To see the second catch,
let's try to write down a dependent type signature for \texttt{filter}.
\begin{verbatim}
filter : {n m : Nat}{E : Element}
       -> Vector n E -> (Exp E -> Exp Bool) -> Vector m E
\end{verbatim}
The problem is that the size of the result
cannot be determined statically. In fact, the only thing we know about
\texttt{m} is that it must be less than or equal to
\texttt{n}. However, we cannot prove this from the code because the
Accelerate implementation computes the length of the result in a quite
subtle way. Moreover, as Accelerate deals with the construction of
abstract syntax most of the time, the value of \texttt{m} is simply
not available at run time of \texttt{filter}. 

One might contemplate that an existential type like
\begin{verbatim}
exists (\ m : Nat -> m <= n -> Vector m E)
\end{verbatim}
could be employed. However, it is not possible to build such an
existential package because the evidence \texttt{m} is not available
when the package is constructed.

However, an alternative encoding of arrays can be
used which is compatible with filtering of elements. The idea of this
encoding is to keep all elements but mark those which are no longer
present because they have been filtered out.
There are several ways of implementing this idea. The simplest
approach is to wrap each element in a maybe type.
pair up each element with a boolean flag that indicates
its presence.\footnote{A \texttt{Maybe} type would be a better option,
  but it has to be coded without using pattern matching.}
\begin{verbatim}
FVector : Nat -> Element -> Set
FVector n E = Vector n (Pair Bool E)
\end{verbatim}

Now filtering becomes quite simple.
\begin{verbatim}
filterF : {n : Nat}{E : Element}
        -> (Exp E -> Exp Bool) -> FVector n E -> FVector n E
filterF {n}{E} pred vec =
  map g vec
  where
  g : Exp (Pair Bool E) -> Exp (Pair Bool E)
  g bx = pair ((fst bx) && p (snd bx)) x
\end{verbatim}
However, mapping becomes more complicated because it either has to
materialize a dummy result for each absent element in the argument
vector or apply the function to absent elements, too.\footnote{This
  complication could be avoided with the \texttt{Maybe} type.}
\begin{verbatim}
mapF : {n : Nat}{E F : Element}
     -> Exp F -> (Exp E -> Exp F) -> FVector n E -> FVector n F
mapF {n}{E}{F} defaultF f vec =
  map g vec
  where
  g : Exp (Pair Bool E) -> Exp (Pair Bool F)
  g bx = if (fst bx) then (pair (fst bx) (f (snd bx)))
                     else (pair (fst bx) defaultF)
\end{verbatim}
On the positive side, some operations can get rid of the absent
elements. In particular, a fold operation which reduces a filtered
vector with a monoid returns a single value. In Accelerate, such a value has type
\texttt{Scalar}, which is a synonym for an array of dimension $0$.
\begin{verbatim}
foldF : {n : Nat}{E : Element}
      -> (Exp E -> Exp E -> Exp E) -> Exp E
      -> FVector n E -> Scalar E
foldF f e vec =
  fold f e (map (\ bx -> if (fst bx) then (snd bx) else e) vec)
\end{verbatim}
Other operations like \texttt{fold1} and the scan operations present
in Accelerate can also be lifted to this representation, but they
retain a notion of absent elements and do not allow to revert to a
non-filtered representation. 

In the end, such a representation may not be a loss on a GPU. As long
as all computations take the same path, all processing elements work
in unison. As soon as there are different paths in the same
computation step, then some elements will be idle for part of the
computation step. So it would be most advantageous to organize work as
uniformly as possible. 


\section{Implementation}
\label{sec:implementation}

Agda is compiled to Haskell.

The compiler supports a Haskell foreign function interface. 

The sophisticated type structure of the Accelerate language is in the
way.

The foreign function interface is not sufficiently expressive so that
Agda types need to be encoded and corresponding structures need to be
build from this encoding in the Haskell code. 

\section{Conclusion}
\label{sec:conclusion}


%
% ---- Bibliography ----
%
\bibliography{local,abbrevs,papers,books,collections,misc,theses}
\bibliographystyle{abbrv}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
